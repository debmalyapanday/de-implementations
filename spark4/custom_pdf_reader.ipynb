{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a99e6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83253df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.datasource import DataSource, DataSourceReader\n",
    "from pyspark.sql.types import StringType, StructField, StructType\n",
    "\n",
    "class PdfDataSource(DataSource):\n",
    "    @classmethod\n",
    "    def name(cls):\n",
    "        return \"pdf\"\n",
    "\n",
    "    def schema(self):\n",
    "        return StructType([\n",
    "            StructField(\"file\", StringType()),\n",
    "            StructField(\"page\", StringType()),\n",
    "            StructField(\"text\", StringType()),\n",
    "            StructField(\"author\", StringType()),\n",
    "            StructField(\"title\", StringType())\n",
    "        ])\n",
    "\n",
    "    def reader(self, schema: StructType):\n",
    "        return PdfDataSourceReader(self.options.get(\"path\"), self.options)\n",
    "\n",
    "class PdfDataSourceReader(DataSourceReader):\n",
    "    def __init__(self, path, options):\n",
    "        self.path = path\n",
    "        self.max_pages = int(options.get(\"max_pages\", -1))\n",
    "        self.extract_tables = options.get(\"extract_tables\", \"false\") == \"true\"\n",
    "\n",
    "    def read(self, partition):\n",
    "        for file_name in os.listdir(self.path):\n",
    "            if file_name.endswith(\".pdf\"):\n",
    "                full_path = os.path.join(self.path, file_name)\n",
    "                with pdfplumber.open(full_path) as pdf:\n",
    "                    metadata = pdf.metadata or {}\n",
    "                    author = metadata.get(\"Author\", \"Unknown\")\n",
    "                    title = metadata.get(\"Title\", file_name)\n",
    "\n",
    "                    for i, page in enumerate(pdf.pages):\n",
    "                        if self.max_pages != -1 and i >= self.max_pages:\n",
    "                            break\n",
    "\n",
    "                        if self.extract_tables:\n",
    "                            tables = page.extract_tables()\n",
    "                            for t_index, table in enumerate(tables):\n",
    "                                yield (file_name, f\"Page {i+1} Table {t_index+1}\", json.dumps(table), author, title)\n",
    "                            text = page.extract_text()\n",
    "                            yield (file_name, f\"Page {i+1} Text\", text, author, title)\n",
    "                        else:\n",
    "                            text = page.extract_text()\n",
    "                            yield (file_name, f\"Page {i+1}\", text, author, title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "418a6980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/09 11:31:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.dataSource.register(PdfDataSource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f1ddfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+-------------------+--------------------+\n",
      "|                file|        page|                text|             author|               title|\n",
      "+--------------------+------------+--------------------+-------------------+--------------------+\n",
      "|big-data-analytic...| Page 1 Text|AWS Whitepaper\\nB...|Amazon Web Services|Big Data Analytic...|\n",
      "|big-data-analytic...| Page 2 Text|Big Data Analytic...|Amazon Web Services|Big Data Analytic...|\n",
      "|big-data-analytic...| Page 3 Text|Big Data Analytic...|Amazon Web Services|Big Data Analytic...|\n",
      "|big-data-analytic...| Page 4 Text|Big Data Analytic...|Amazon Web Services|Big Data Analytic...|\n",
      "|big-data-analytic...| Page 5 Text|Big Data Analytic...|Amazon Web Services|Big Data Analytic...|\n",
      "|big-data-analytic...| Page 6 Text|Big Data Analytic...|Amazon Web Services|Big Data Analytic...|\n",
      "|big-data-analytic...| Page 7 Text|Big Data Analytic...|Amazon Web Services|Big Data Analytic...|\n",
      "|big-data-analytic...| Page 8 Text|Big Data Analytic...|Amazon Web Services|Big Data Analytic...|\n",
      "|big-data-analytic...| Page 9 Text|Big Data Analytic...|Amazon Web Services|Big Data Analytic...|\n",
      "|big-data-analytic...|Page 10 Text|Big Data Analytic...|Amazon Web Services|Big Data Analytic...|\n",
      "+--------------------+------------+--------------------+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"pdf\") \\\n",
    "    .option(\"path\", \"data/\") \\\n",
    "    .option(\"max_pages\", 10) \\\n",
    "    .option(\"extract_tables\", \"true\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26b5ba4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big Data Analytics Options on AWS AWS Whitepaper\n",
      "The AWS advantage in big data analytics\n",
      "Analyzing large datasets requires significant compute capacity that can vary in size, based on\n",
      "the amount of input data and the type of analysis. This characteristic of big data workloads is\n",
      "ideally suited to the pay-as-you-go cloud computing model, where applications can easily scale\n",
      "up and down based on demand. As requirements change, you can easily resize your environment\n",
      "(horizontally or vertically) on AWS to meet your needs, without having to wait for additional\n",
      "hardware or over-investing to provision enough capacity.\n",
      "For mission-critical applications on a more traditional infrastructure, system designers have no\n",
      "choice but to over-provision, because a surge in additional data due to an increase in business\n",
      "needs must be something the system can handle. By contrast, on AWS, you can provision more\n",
      "capacity and compute in a matter of minutes, meaning that your big data applications grow and\n",
      "shrink as demand dictates, and your system runs as close to optimal efficiency as possible.\n",
      "In addition, you get flexible computing on a global infrastructure with access to the many different\n",
      "geographic Regions that AWS offers, along with the ability to use other scalable services that\n",
      "augment to build sophisticated big data applications. These other services include:\n",
      "• Amazon Simple Storage Service (Amazon S3) to store data\n",
      "• AWS Glue to orchestrate jobs to move and transform the data easily\n",
      "• AWS IoT, which lets connected devices interact with cloud applications and other connected\n",
      "devices\n",
      "As the amount of data being generated continues to grow, AWS has many options to get that\n",
      "data to the cloud, including secure devices like AWS Snow Family to accelerate petabyte-scale\n",
      "data transfers, delivery streams with Amazon Data Firehose to load streaming data continuously,\n",
      "migrating databases using AWS Database Migration Service, and scalable private connections\n",
      "through AWS Direct Connect.\n",
      "As mobile continues to rapidly grow in usage, you can use the suite of services within the AWS\n",
      "Mobile Hub to collect and measure app usage and data, or export that data to another service for\n",
      "further custom analysis.\n",
      "These capabilities of AWS make it an ideal fit for solving big data problems, and many customers\n",
      "have implemented successful big data analytics workloads on AWS. For more information about\n",
      "case studies, see Big Data Customer Success Stories.\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "table_row = df.filter(df.page == \"Page 10 Text\").collect()\n",
    "print(table_row[0]['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
